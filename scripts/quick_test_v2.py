"""
Quick Test Suite for IR System v2
å¿«é€Ÿæ¸¬è©¦å¥—ä»¶ - ä¿®æ­£ç‰ˆï¼Œä½¿ç”¨æ­£ç¢ºçš„APIæ¥å£
"""

import sys
from pathlib import Path

# Add parent directory to path
_parent_dir = Path(__file__).parent.parent
if str(_parent_dir) not in sys.path:
    sys.path.insert(0, str(_parent_dir))

# Import IR modules
from src.ir.text.chinese_tokenizer import ChineseTokenizer
from src.ir.index.inverted_index import InvertedIndex
from src.ir.index.positional_index import PositionalIndex
from src.ir.retrieval.vsm import VectorSpaceModel
from src.ir.retrieval.bm25 import BM25Ranker
from src.ir.retrieval.language_model_retrieval import LanguageModelRetrieval
from src.ir.retrieval.bim import BinaryIndependenceModel
from src.ir.retrieval.boolean import BooleanQueryEngine
from src.ir.ranking.hybrid import HybridRanker
from src.ir.langmodel.ngram import NGramModel
from src.ir.langmodel.collocation import CollocationExtractor
from src.ir.ranking.rocchio import RocchioExpander
from src.ir.cluster.doc_cluster import DocumentClusterer
from src.ir.summarize.static import StaticSummarizer
from src.ir.summarize.dynamic import KWICGenerator
from src.ir.index.compression import VByteEncoder, GammaEncoder, DeltaEncoder

print("=" * 80)
print("IR ç³»çµ±å¿«é€Ÿæ¸¬è©¦å¥—ä»¶ v2 (Quick Test Suite)")
print("=" * 80)
print()

# Test documents
test_docs = [
    "è³‡è¨Šæª¢ç´¢æ˜¯è¨ˆç®—æ©Ÿç§‘å­¸çš„é‡è¦åˆ†æ”¯",
    "å‘é‡ç©ºé–“æ¨¡å‹ä½¿ç”¨TF-IDFæ¬Šé‡è¨ˆç®—ç›¸ä¼¼åº¦",
    "BM25æ˜¯ä¸€ç¨®æ©Ÿç‡æ’åºå‡½æ•¸å»£æ³›æ‡‰ç”¨æ–¼æœå°‹å¼•æ“",
    "èªè¨€æ¨¡å‹å¯ä»¥ç”¨æ–¼æ–‡æœ¬æª¢ç´¢å’Œç”Ÿæˆ",
    "å¸ƒæ—æª¢ç´¢æ”¯æ´AND OR NOTç­‰é‚è¼¯é‹ç®—å­",
    "å€’æ’ç´¢å¼•æ˜¯è³‡è¨Šæª¢ç´¢ç³»çµ±çš„æ ¸å¿ƒæ•¸æ“šçµæ§‹",
    "è©å½™å…±ç¾åˆ†æå¯ä»¥ç™¼ç¾æœ‰æ„ç¾©çš„è©çµ„",
    "æ–‡æª”åˆ†ç¾¤å¯ä»¥çµ„ç¹”å¤§é‡æ–‡æª”é›†åˆ",
    "æŸ¥è©¢æ“´å±•ä½¿ç”¨Rocchioæ¼”ç®—æ³•æ”¹å–„æª¢ç´¢æ•ˆæœ",
    "N-gramèªè¨€æ¨¡å‹ä¼°è¨ˆè©åºåˆ—çš„æ©Ÿç‡"
]

# Results tracking
tests_passed = 0
tests_failed = 0

def test_result(name, passed, details=""):
    global tests_passed, tests_failed
    if passed:
        tests_passed += 1
        print(f"âœ… PASS - {name}")
    else:
        tests_failed += 1
        print(f"âŒ FAIL - {name}")
    if details:
        print(f"       {details}")

# Test 1: Tokenizer
print("ã€1. åˆ†è©å™¨ã€‘")
try:
    tokenizer = ChineseTokenizer(engine='jieba')
    test_text = "è³‡è¨Šæª¢ç´¢ç³»çµ±"
    tokens = tokenizer.tokenize(test_text)
    test_result("Jieba åˆ†è©", len(tokens) > 0, f"'{test_text}' â†’ {tokens}")
except Exception as e:
    test_result("Jieba åˆ†è©", False, str(e))
print()

# Test 2: Inverted Index
print("ã€2. å€’æ’ç´¢å¼•ã€‘")
try:
    inv_idx = InvertedIndex(tokenizer=tokenizer.tokenize)
    inv_idx.build(test_docs)
    term_count = len(inv_idx.index)
    test_result("å€’æ’ç´¢å¼•å»ºç«‹", term_count > 0, f"{len(test_docs)} docs, {term_count} terms")
except Exception as e:
    test_result("å€’æ’ç´¢å¼•å»ºç«‹", False, str(e))
print()

# Test 3: Positional Index
print("ã€3. ä½ç½®ç´¢å¼•ã€‘")
try:
    pos_idx = PositionalIndex(tokenizer=tokenizer.tokenize)
    pos_idx.build(test_docs)
    positions = pos_idx.get_positions("æª¢ç´¢", 0)
    test_result("ä½ç½®ç´¢å¼•", positions is not None, f"'æª¢ç´¢' åœ¨ doc 0: positions={positions}")
except Exception as e:
    test_result("ä½ç½®ç´¢å¼•", False, str(e))
print()

# Test 4: VSM
print("ã€4. å‘é‡ç©ºé–“æ¨¡å‹ (VSM)ã€‘")
try:
    vsm = VectorSpaceModel()
    vsm.build_index(test_docs)
    query = "è³‡è¨Šæª¢ç´¢"
    result = vsm.search(query, topk=3)
    test_result("VSM æª¢ç´¢", len(result) > 0, f"æŸ¥è©¢ '{query}': {len(result)} çµæœ")
    for i, (doc_id, score) in enumerate(result[:2], 1):
        print(f"       #{i}. Doc {doc_id}: score={score:.4f}")
except Exception as e:
    test_result("VSM æª¢ç´¢", False, str(e))
print()

# Test 5: BM25
print("ã€5. BM25 æ’åºã€‘")
try:
    bm25 = BM25Ranker(tokenizer=tokenizer.tokenize, k1=1.5, b=0.75)
    bm25.build_index(test_docs)
    result = bm25.search(query, topk=3)
    test_result("BM25 æª¢ç´¢", len(result) > 0, f"æŸ¥è©¢ '{query}': {len(result)} çµæœ")
    for i, (doc_id, score) in enumerate(result[:2], 1):
        print(f"       #{i}. Doc {doc_id}: score={score:.4f}")
except Exception as e:
    test_result("BM25 æª¢ç´¢", False, str(e))
print()

# Test 6: Language Model
print("ã€6. èªè¨€æ¨¡å‹æª¢ç´¢ã€‘")
try:
    lm = LanguageModelRetrieval(tokenizer=tokenizer.tokenize, smoothing='dirichlet', mu_param=2000)
    lm.build_index(test_docs)
    result = lm.search(query, topk=3)
    test_result("LM æª¢ç´¢", len(result) > 0, f"æŸ¥è©¢ '{query}': {len(result)} çµæœ")
    for i, (doc_id, score) in enumerate(result[:2], 1):
        print(f"       #{i}. Doc {doc_id}: score={score:.4f}")
except Exception as e:
    test_result("LM æª¢ç´¢", False, str(e))
print()

# Test 7: BIM
print("ã€7. äºŒå…ƒç¨ç«‹æ¨¡å‹ (BIM)ã€‘")
try:
    bim = BinaryIndependenceModel(tokenizer=tokenizer.tokenize, use_idf=True)
    bim.build_index(test_docs)
    result = bim.search(query, topk=3)
    test_result("BIM æª¢ç´¢", len(result) > 0, f"æŸ¥è©¢ '{query}': {len(result)} çµæœ")
except Exception as e:
    test_result("BIM æª¢ç´¢", False, str(e))
print()

# Test 8: Boolean Query
print("ã€8. å¸ƒæ—æª¢ç´¢ã€‘")
try:
    boolean_engine = BooleanQueryEngine(inverted_index=inv_idx, positional_index=pos_idx)
    bool_query = "è³‡è¨Š AND æª¢ç´¢"
    result = boolean_engine.search(bool_query)
    test_result("Boolean AND", len(result) > 0, f"æŸ¥è©¢ '{bool_query}': {result}")
except Exception as e:
    test_result("Boolean AND", False, str(e))
print()

# Test 9: Hybrid Ranking
print("ã€9. æ··åˆæ’åºã€‘")
try:
    rankers = {'bm25': bm25, 'vsm': vsm, 'lm': lm}
    hybrid = HybridRanker(rankers=rankers, fusion_method='rrf')
    result = hybrid.search(query, topk=3, ranker_topk=10)
    test_result("Hybrid RRF", len(result) > 0, f"èåˆ3å€‹rankers: {len(result)} çµæœ")
    for i, (doc_id, score) in enumerate(result[:2], 1):
        print(f"       #{i}. Doc {doc_id}: score={score:.4f}")
except Exception as e:
    test_result("Hybrid RRF", False, str(e))
print()

# Test 10: N-gram Model
print("ã€10. N-gram èªè¨€æ¨¡å‹ã€‘")
try:
    ngram = NGramModel(n=2, smoothing='jm', lambda_param=0.7)
    ngram.train(test_docs)
    test_sentence = "è³‡è¨Šæª¢ç´¢"
    prob = ngram.probability(test_sentence)
    perplexity = ngram.perplexity(test_sentence)
    test_result("N-gram", prob > 0, f"'{test_sentence}': prob={prob:.2e}, perplexity={perplexity:.2f}")
except Exception as e:
    test_result("N-gram", False, str(e))
print()

# Test 11: Collocation
print("ã€11. è©å½™å…±ç¾åˆ†æã€‘")
try:
    colloc = CollocationExtractor(tokenizer=tokenizer.tokenize, min_freq=1, window_size=2)
    colloc.build(test_docs)
    collocations = colloc.extract_collocations(measure='pmi', topk=5)
    test_result("Collocation PMI", len(collocations) > 0, f"ç™¼ç¾ {len(collocations)} å€‹å…±ç¾è©çµ„")
    for i, col in enumerate(collocations[:2], 1):
        print(f"       #{i}. {col['bigram']}: PMI={col['pmi']:.2f}, freq={col['freq']}")
except Exception as e:
    test_result("Collocation PMI", False, str(e))
print()

# Test 12: Rocchio Query Expansion
print("ã€12. æŸ¥è©¢æ“´å±• (Rocchio)ã€‘")
try:
    rocchio = RocchioExpander(alpha=1.0, beta=0.75, gamma=0.15, max_expansion_terms=3)
    query_vec = {"è³‡è¨Š": 0.8, "æª¢ç´¢": 0.6}
    rel_vecs = [
        {"è³‡è¨Š": 0.5, "æª¢ç´¢": 0.7, "ç³»çµ±": 0.3},
        {"è³‡è¨Š": 0.6, "æ–‡æª”": 0.4}
    ]
    expanded = rocchio.expand_query(query_vec, rel_vecs)
    test_result("Rocchio æ“´å±•", len(expanded.all_terms) >= len(expanded.original_terms),
                f"{len(expanded.original_terms)} â†’ {len(expanded.all_terms)} terms")
    print(f"       åŸå§‹: {expanded.original_terms}")
    print(f"       æ“´å±•: {expanded.expanded_terms}")
except Exception as e:
    test_result("Rocchio æ“´å±•", False, str(e))
print()

# Test 13: Document Clustering
print("ã€13. æ–‡æª”åˆ†ç¾¤ã€‘")
try:
    clusterer = DocumentClusterer(tokenizer=tokenizer.tokenize, method='kmeans')
    clusters = clusterer.cluster(test_docs, n_clusters=3)
    test_result("K-means åˆ†ç¾¤", len(clusters) > 0, f"åˆ†æˆ {len(clusters)} ç¾¤")
    for i, cluster in enumerate(clusters):
        print(f"       Cluster {i}: {cluster['size']} docs")
except Exception as e:
    test_result("K-means åˆ†ç¾¤", False, str(e))
print()

# Test 14: Summarization
print("ã€14. æ–‡æª”æ‘˜è¦ã€‘")
try:
    summarizer = StaticSummarizer()
    doc = test_docs[0]
    summary = summarizer.lead_k_summarization(doc, k=1)
    test_result("Lead-K æ‘˜è¦", len(summary) > 0, f"æ‘˜è¦é•·åº¦: {len(summary)}")
except Exception as e:
    test_result("Lead-K æ‘˜è¦", False, str(e))
print()

# Test 15: KWIC
print("ã€15. KWIC (é—œéµè©ä¸Šä¸‹æ–‡)ã€‘")
try:
    kwic = KWICGenerator(window_size=5)
    doc = "è³‡è¨Šæª¢ç´¢æ˜¯è¨ˆç®—æ©Ÿç§‘å­¸çš„é‡è¦åˆ†æ”¯ï¼Œæª¢ç´¢ç³»çµ±ç”¨æ–¼æŸ¥æ‰¾è³‡è¨Š"
    keyword = "æª¢ç´¢"
    result = kwic.generate(doc, keyword)
    test_result("KWIC ç”Ÿæˆ", len(result['contexts']) > 0,
                f"æ‰¾åˆ° {len(result['contexts'])} å€‹ '{keyword}' ä¸Šä¸‹æ–‡")
    for ctx in result['contexts'][:1]:
        print(f"       ...{ctx['left']}ã€{ctx['keyword']}ã€‘{ctx['right']}...")
except Exception as e:
    test_result("KWIC ç”Ÿæˆ", False, str(e))
print()

# Test 16: Index Compression
print("ã€16. ç´¢å¼•å£“ç¸®ã€‘")
test_doc_ids = [3, 7, 10, 15, 22, 30]

try:
    vbyte = VByteEncoder()
    encoded = vbyte.encode_gaps(test_doc_ids)
    decoded = vbyte.decode_gaps(encoded)
    test_result("VByte å£“ç¸®", decoded == test_doc_ids,
                f"{len(test_doc_ids)} ids â†’ {len(encoded)} bytes")
except Exception as e:
    test_result("VByte å£“ç¸®", False, str(e))

try:
    gamma = GammaEncoder()
    encoded = gamma.encode_gaps(test_doc_ids)
    decoded = gamma.decode_gaps(encoded)
    test_result("Gamma å£“ç¸®", decoded == test_doc_ids, f"ç·¨ç¢¼/è§£ç¢¼æ­£ç¢º")
except Exception as e:
    test_result("Gamma å£“ç¸®", False, str(e))

try:
    delta = DeltaEncoder()
    encoded = delta.encode_gaps(test_doc_ids)
    decoded = delta.decode_gaps(encoded)
    test_result("Delta å£“ç¸®", decoded == test_doc_ids, f"ç·¨ç¢¼/è§£ç¢¼æ­£ç¢º")
except Exception as e:
    test_result("Delta å£“ç¸®", False, str(e))
print()

# Summary
print("=" * 80)
print("æ¸¬è©¦ç¸½çµ")
print("=" * 80)
total_tests = tests_passed + tests_failed
pass_rate = (tests_passed / total_tests * 100) if total_tests > 0 else 0
print(f"ç¸½æ¸¬è©¦æ•¸: {total_tests}")
print(f"é€šé: {tests_passed} âœ…")
print(f"å¤±æ•—: {tests_failed} âŒ")
print(f"æˆåŠŸç‡: {pass_rate:.1f}%")
print()

if tests_failed == 0:
    print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼ç³»çµ±é‹ä½œæ­£å¸¸ï¼")
else:
    print(f"âš ï¸  æœ‰ {tests_failed} å€‹æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¸Šè¿°éŒ¯èª¤è¨Šæ¯")
