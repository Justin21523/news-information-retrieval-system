#!/usr/bin/env python
"""
CKIP Tokenizer Threading Benchmark

Benchmark CKIP tokenization performance with different thread counts.
Helps determine optimal threading configuration for your system.

Usage:
    python scripts/benchmark_ckip_threads.py
    python scripts/benchmark_ckip_threads.py --threads 16 32
    python scripts/benchmark_ckip_threads.py --sentences 100

Author: Information Retrieval System
"""

import sys
import time
import argparse
import logging
from pathlib import Path
from typing import List

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.ir.text.ckip_tokenizer_optimized import CKIPTokenizerOptimized


def generate_test_sentences(count: int = 100) -> List[str]:
    """
    Generate test sentences for benchmarking.

    Args:
        count: Number of sentences to generate

    Returns:
        List of test sentences
    """
    base_sentences = [
        "å°ç£æ˜¯ä¸€å€‹ç¾éº—çš„å³¶å¶¼ï¼Œæ“æœ‰è±å¯Œçš„è‡ªç„¶è³‡æºå’Œå¤šå…ƒçš„æ–‡åŒ–ã€‚",
        "äººå·¥æ™ºæ…§æŠ€è¡“åœ¨è¿‘å¹´ä¾†å–å¾—äº†çªç ´æ€§çš„é€²å±•ï¼Œæ·±åº¦å­¸ç¿’æ¨¡å‹è¶Šä¾†è¶Šå¼·å¤§ã€‚",
        "æ–°èå ±å°æŒ‡å‡ºï¼Œä»Šå¹´ç¶“æ¿Ÿæˆé•·ç‡é”åˆ°äº†é æœŸç›®æ¨™ï¼Œç”¢æ¥­å¾©ç”¦æ…‹å‹¢è‰¯å¥½ã€‚",
        "æ°£å€™è®Šé·å•é¡Œæ—¥ç›Šåš´é‡ï¼Œå„åœ‹æ”¿åºœå¿…é ˆæ¡å–æ›´ç©æ¥µçš„è¡Œå‹•ä¾†æ¸›å°‘ç¢³æ’æ”¾ã€‚",
        "ç§‘æŠ€å…¬å¸æŒçºŒæŠ•è³‡ç ”ç™¼ï¼Œå¸Œæœ›èƒ½å¤ é–‹ç™¼å‡ºæ›´å‰µæ–°çš„ç”¢å“å’Œæœå‹™ã€‚",
        "æ•™è‚²æ”¹é©æ˜¯ç¤¾æœƒé€²æ­¥çš„é—œéµï¼Œéœ€è¦æ”¿åºœã€å­¸æ ¡å’Œå®¶é•·å…±åŒåŠªåŠ›ã€‚",
        "å¥åº·é†«ç™‚é«”ç³»é¢è‡¨è«¸å¤šæŒ‘æˆ°ï¼ŒåŒ…æ‹¬äººå£è€åŒ–å’Œé†«ç™‚è³‡æºåˆ†é…ä¸å‡ã€‚",
        "é‡‘èå¸‚å ´æ³¢å‹•åŠ åŠ‡ï¼ŒæŠ•è³‡äººéœ€è¦æ›´è¬¹æ…åœ°è©•ä¼°é¢¨éšªå’Œæ©Ÿæœƒã€‚",
        "ç’°ä¿æ„è­˜æŠ¬é ­ï¼Œè¶Šä¾†è¶Šå¤šä¼æ¥­é–‹å§‹é‡è¦–æ°¸çºŒç™¼å±•å’Œç¤¾æœƒè²¬ä»»ã€‚",
        "æ•¸ä½è½‰å‹å·²æˆç‚ºä¼æ¥­ç”Ÿå­˜çš„å¿…è¦æ¢ä»¶ï¼Œå‚³çµ±ç”¢æ¥­ä¹Ÿéœ€è¦ç©æ¥µé©æ‡‰ã€‚",
        "éƒ½å¸‚åŒ–é€²ç¨‹åŠ é€Ÿï¼ŒåŸå¸‚è¦åŠƒå¿…é ˆå…¼é¡§ç¶“æ¿Ÿç™¼å±•å’Œç”Ÿæ´»å“è³ªã€‚",
        "ç¶²è·¯å®‰å…¨å¨è„…æŒçºŒå¢åŠ ï¼Œå€‹äººå’Œä¼æ¥­éƒ½éœ€è¦åŠ å¼·è³‡è¨Šä¿è­·æªæ–½ã€‚",
        "å†ç”Ÿèƒ½æºç”¢æ¥­å¿«é€Ÿç™¼å±•ï¼Œå¤ªé™½èƒ½å’Œé¢¨åŠ›ç™¼é›»æˆæœ¬æŒçºŒé™ä½ã€‚",
        "äººæ‰åŸ¹è‚²æ˜¯åœ‹å®¶ç«¶çˆ­åŠ›çš„æ ¹æœ¬ï¼Œæ•™è‚²æŠ•è³‡ä¸å®¹å¿½è¦–ã€‚",
        "ç–«æƒ…æ”¹è®Šäº†å·¥ä½œå‹æ…‹ï¼Œé è·è¾¦å…¬æˆç‚ºæ–°å¸¸æ…‹ã€‚",
    ]

    # Repeat to reach desired count
    sentences = []
    while len(sentences) < count:
        sentences.extend(base_sentences)

    return sentences[:count]


def benchmark_threading(thread_counts: List[int],
                        num_sentences: int = 100,
                        batch_size: int = 512) -> dict:
    """
    Benchmark CKIP tokenization with different thread counts.

    Args:
        thread_counts: List of thread counts to test
        num_sentences: Number of sentences to process
        batch_size: Batch size for tokenization

    Returns:
        Dictionary with benchmark results
    """
    logger = logging.getLogger(__name__)

    # Generate test data
    logger.info(f"Generating {num_sentences} test sentences...")
    test_sentences = generate_test_sentences(num_sentences)

    results = {}

    for threads in thread_counts:
        logger.info(f"\n{'='*80}")
        logger.info(f"Benchmarking with {threads} threads")
        logger.info(f"{'='*80}")

        # Reset singleton for each test
        CKIPTokenizerOptimized._instance = None
        CKIPTokenizerOptimized._initialized = False

        try:
            # Initialize tokenizer
            logger.info("Initializing CKIP tokenizer...")
            init_start = time.time()
            tokenizer = CKIPTokenizerOptimized(num_threads=threads)
            init_time = time.time() - init_start
            logger.info(f"  Initialization: {init_time:.4f}s")

            # Warm-up run
            logger.info("Warm-up run...")
            _ = tokenizer.tokenize_batch(test_sentences[:10], batch_size=batch_size)

            # Benchmark run
            logger.info(f"Processing {num_sentences} sentences (batch_size={batch_size})...")
            start_time = time.time()
            token_results = tokenizer.tokenize_batch(test_sentences, batch_size=batch_size)
            elapsed = time.time() - start_time

            # Calculate statistics
            total_tokens = sum(len(tokens) for tokens in token_results)
            throughput = num_sentences / elapsed
            tokens_per_sec = total_tokens / elapsed

            results[threads] = {
                'threads': threads,
                'sentences': num_sentences,
                'batch_size': batch_size,
                'init_time': init_time,
                'processing_time': elapsed,
                'total_tokens': total_tokens,
                'throughput': throughput,  # sentences/sec
                'tokens_per_sec': tokens_per_sec,
                'avg_time_per_sentence': elapsed / num_sentences * 1000,  # ms
            }

            logger.info(f"\nResults:")
            logger.info(f"  Processing time: {elapsed:.4f}s")
            logger.info(f"  Throughput: {throughput:.2f} sentences/sec")
            logger.info(f"  Token throughput: {tokens_per_sec:.2f} tokens/sec")
            logger.info(f"  Total tokens: {total_tokens:,}")
            logger.info(f"  Avg per sentence: {elapsed / num_sentences * 1000:.2f}ms")

            # Get tokenizer stats
            stats = tokenizer.get_stats()
            logger.info(f"\nTokenizer configuration:")
            for k, v in stats.items():
                logger.info(f"  {k}: {v}")

        except Exception as e:
            logger.error(f"Error with {threads} threads: {e}", exc_info=True)
            results[threads] = {'error': str(e)}

    return results


def print_comparison(results: dict):
    """
    Print comparison table of benchmark results.

    Args:
        results: Dictionary with benchmark results
    """
    print("\n" + "="*100)
    print("CKIP TOKENIZATION PERFORMANCE COMPARISON")
    print("="*100)
    print()

    # Table header
    header = f"{'Threads':<10} {'Time (s)':<12} {'Throughput':<20} {'Tokens/sec':<18} {'Speedup':<12}"
    print(header)
    print("-" * 100)

    # Get baseline (first result)
    baseline_threads = min(results.keys())
    baseline_time = results[baseline_threads]['processing_time']

    # Print results
    for threads in sorted(results.keys()):
        r = results[threads]

        if 'error' in r:
            print(f"{threads:<10} ERROR: {r['error']}")
            continue

        time_val = r['processing_time']
        throughput = r['throughput']
        tokens_per_sec = r['tokens_per_sec']
        speedup = baseline_time / time_val

        print(f"{threads:<10} {time_val:<12.4f} {throughput:<20.2f} {tokens_per_sec:<18.2f} {speedup:<12.2f}x")

    print("-" * 100)

    # Find best configuration
    best_threads = max(results.keys(), key=lambda k: results[k].get('throughput', 0))
    best_throughput = results[best_threads]['throughput']
    best_speedup = baseline_time / results[best_threads]['processing_time']

    print(f"\nğŸ† Best configuration: {best_threads} threads")
    print(f"   Throughput: {best_throughput:.2f} sentences/sec")
    print(f"   Speedup: {best_speedup:.2f}x over baseline ({baseline_threads} threads)")
    print("="*100 + "\n")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description='Benchmark CKIP tokenization with different thread counts'
    )
    parser.add_argument(
        '--threads',
        type=int,
        nargs='+',
        default=[8, 16, 24, 32],
        help='Thread counts to test (default: 8 16 24 32)'
    )
    parser.add_argument(
        '--sentences',
        type=int,
        default=100,
        help='Number of sentences to process (default: 100)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=512,
        help='Batch size for tokenization (default: 512)'
    )
    parser.add_argument(
        '--verbose',
        '-v',
        action='store_true',
        help='Verbose output (DEBUG level)'
    )

    args = parser.parse_args()

    # Setup logging
    level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    logger = logging.getLogger(__name__)

    logger.info("="*80)
    logger.info("CKIP Tokenizer Threading Benchmark")
    logger.info("="*80)
    logger.info(f"Test configuration:")
    logger.info(f"  Thread counts: {args.threads}")
    logger.info(f"  Sentences: {args.sentences}")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info("="*80)

    # Run benchmark
    results = benchmark_threading(
        thread_counts=args.threads,
        num_sentences=args.sentences,
        batch_size=args.batch_size
    )

    # Print comparison
    print_comparison(results)

    # Recommendation
    import multiprocessing
    cpu_count = multiprocessing.cpu_count()
    print(f"ğŸ’¡ System information:")
    print(f"   Available CPU threads: {cpu_count}")
    print(f"\nğŸ’¡ Recommendation:")
    print(f"   For maximum speed, use: --num-threads {cpu_count}")
    print(f"   For balanced performance: --num-threads {cpu_count // 2}")
    print()


if __name__ == '__main__':
    main()
