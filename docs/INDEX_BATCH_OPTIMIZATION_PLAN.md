# ç´¢å¼•å»ºç«‹æ‰¹æ¬¡è™•ç†å„ªåŒ–æ–¹æ¡ˆ

## ğŸ“‹ å•é¡Œåˆ†æ

### ç•¶å‰å•é¡Œç™¼ç¾ (2025-11-20)

åœ¨åˆ†æ 50,000 ç¯‡æ–‡æª”çš„ç´¢å¼•å»ºç«‹éç¨‹ä¸­ï¼Œç™¼ç¾ä»¥ä¸‹æ•ˆèƒ½ç“¶é ¸ï¼š

#### 1. **å–®æ–‡æª”è™•ç†ç“¶é ¸**
**ä½ç½®**: `src/ir/index/incremental_builder.py` (line 111-128)

```python
def _ckip_tokenizer(self, text: str) -> List[str]:
    """å–®å€‹æ–‡æª”çš„åˆ†è©è™•ç†"""
    return self.ckip_tokenizer.tokenize(
        text,
        filter_stopwords=True,
        min_length=2
    )
```

**å•é¡Œ**:
- æ¯æ¬¡åªè™•ç† 1 å€‹æ–‡æª”
- ç„¡æ³•åˆ©ç”¨ CKIP çš„æ‰¹æ¬¡è™•ç† API (`tokenize_batch`)
- å°è‡´å¤§é‡é€²åº¦æ¢è¼¸å‡º (136,832 æ¬¡)

#### 2. **ç´¢å¼•å»ºç«‹æµç¨‹**
**ä½ç½®**: `src/ir/search/unified_search.py` (line 218-250)

```python
for doc in self.doc_reader.read_directory(...):
    # é€å€‹è™•ç†æ–‡æª”
    self.index_builder.add_document(doc)  # æ¯æ¬¡åªè™•ç† 1 å€‹
```

**å•é¡Œ**:
- é€ä¸€è™•ç†æ–‡æª”ï¼Œç„¡æ‰¹æ¬¡åŒ–
- æœªå……åˆ†åˆ©ç”¨ 32 åŸ·è¡Œç·’èƒ½åŠ›
- I/O å’Œè¨ˆç®—æœªèƒ½æœ‰æ•ˆé‡ç–Š

#### 3. **å¯¦éš›æ•ˆèƒ½æ•¸æ“š**

| æŒ‡æ¨™ | ç•¶å‰å€¼ | ç†æƒ³å€¼ | å·®è· |
|------|--------|--------|------|
| **è™•ç†æ™‚é–“** | 4.5+ å°æ™‚ | ~1 å°æ™‚ | **4.5x æ…¢** |
| **CPU åˆ©ç”¨ç‡** | 29-30 æ ¸å¿ƒ (2961%) | 30-32 æ ¸å¿ƒ | æ­£å¸¸ |
| **Tokenization æ¬¡æ•¸** | 136,832 æ¬¡ (é€²åº¦æ¢æ—¥èªŒ) | 50,000 æ¬¡ | **2.7x å¤š** |
| **æ‰¹æ¬¡å¤§å°** | 1 (å–®æ–‡æª”) | 100-500 (æ‰¹æ¬¡) | **æœªæ‰¹æ¬¡åŒ–** |

---

## ğŸ¯ å„ªåŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ A: æ‰¹æ¬¡è™•ç†å„ªåŒ– (æ¨è–¦)

#### æ ¸å¿ƒæ”¹é€²

**1. ä¿®æ”¹ IncrementalIndexBuilder**

åœ¨ `src/ir/index/incremental_builder.py` æ–°å¢æ‰¹æ¬¡è™•ç†æ–¹æ³•ï¼š

```python
def add_documents_batch(self,
                       docs: List[NewsDocument],
                       ckip_batch_size: int = 512) -> List[Tuple[bool, str]]:
    """
    Batch process multiple documents for efficient CKIP tokenization.

    Args:
        docs: List of NewsDocument objects
        ckip_batch_size: Batch size for CKIP processing

    Returns:
        List of (success, message) tuples

    Complexity:
        Time: O(N * T_avg / B) where B is batch size
        Space: O(B * T_avg) for batch buffer
    """
    results = []

    # Prepare texts for batch tokenization
    texts = []
    valid_docs = []

    for doc in docs:
        # Deduplication check
        if self.use_dedup:
            is_unique, dup_id = self.dedup.add_document(
                doc.get_full_text(),
                doc.content_hash,
                use_exact=True,
                use_fuzzy=True
            )
            if not is_unique:
                self.docs_duplicates += 1
                results.append((False, f"Duplicate"))
                continue

        texts.append(doc.get_full_text())
        valid_docs.append(doc)
        self.docs_processed += 1

    # Batch tokenize all texts at once
    if texts:
        try:
            from ..text.ckip_tokenizer_optimized import get_optimized_tokenizer

            # Use optimized tokenizer with batch processing
            tokenizer = get_optimized_tokenizer(num_threads=32)
            all_tokens = tokenizer.tokenize_batch(
                texts,
                batch_size=ckip_batch_size,
                filter_stopwords=True,
                min_length=2
            )

            # Add to index
            for doc, tokens in zip(valid_docs, all_tokens):
                doc_id = self.index.add_document_from_tokens(
                    tokens=tokens,
                    metadata=doc.to_dict()
                )
                doc.doc_id = doc_id
                self.docs_indexed += 1
                results.append((True, f"Indexed as doc_id={doc_id}"))

        except Exception as e:
            self.logger.error(f"Batch processing error: {e}")
            # Fallback to single-doc processing
            for doc in valid_docs:
                result = self.add_document(doc)
                results.append(result)

    return results
```

**2. ä¿®æ”¹ UnifiedSearchEngine**

åœ¨ `src/ir/search/unified_search.py` æ”¹ç”¨æ‰¹æ¬¡è™•ç†ï¼š

```python
def build_index_from_jsonl(self, data_dir: str,
                          pattern: str = "*.jsonl",
                          limit: Optional[int] = None,
                          doc_batch_size: int = 100) -> Dict[str, Any]:
    """
    Build index with batch processing optimization.

    Args:
        data_dir: Data directory
        pattern: File pattern
        limit: Document limit
        doc_batch_size: Number of docs to batch process
    """
    self.logger.info(f"Building index with batch processing (batch_size={doc_batch_size})...")

    field_docs = []
    doc_buffer = []
    processed_count = 0

    for doc in self.doc_reader.read_directory(
        directory=data_dir,
        pattern=pattern,
        total_limit=limit
    ):
        doc_buffer.append(doc)

        # Process in batches
        if len(doc_buffer) >= doc_batch_size:
            # Batch process
            results = self.index_builder.add_documents_batch(
                doc_buffer,
                ckip_batch_size=512  # CKIP internal batch size
            )

            # Track successful documents
            for i, (success, _) in enumerate(results):
                if success:
                    indexed_doc = doc_buffer[i]
                    doc_id = self.index_builder.docs_indexed - len([r for r in results if r[0]]) + i

                    # Store metadata
                    self.doc_metadata[doc_id] = {
                        'title': indexed_doc.title,
                        'content': indexed_doc.content,
                        'source': indexed_doc.source,
                        'category': indexed_doc.category,
                        'published_at': indexed_doc.published_at,
                        'url': indexed_doc.url,
                        'author': indexed_doc.author
                    }

                    # Prepare field indexing
                    field_docs.append({
                        'doc_id': doc_id,
                        'title': indexed_doc.title,
                        'content': indexed_doc.content,
                        'source': indexed_doc.source,
                        'category': indexed_doc.category,
                        'published_date': indexed_doc.published_at,
                        'author': indexed_doc.author or '',
                        'url': indexed_doc.url or ''
                    })

            processed_count += len(doc_buffer)
            if processed_count % 1000 == 0:
                self.logger.info(f"Processed {processed_count} documents...")

            doc_buffer = []

    # Process remaining documents
    if doc_buffer:
        results = self.index_builder.add_documents_batch(doc_buffer, ckip_batch_size=512)
        # ... (same as above)

    # Build field indexes and retrieval models
    self.field_indexer.build(field_docs)
    self._build_retrieval_models()

    # ... (rest of the code)
```

---

### æ–¹æ¡ˆ B: æ¸›å°‘æ—¥èªŒè¼¸å‡º (å¿«é€Ÿä¿®å¾©)

å¦‚æœä¸æƒ³å¤§å¹…ä¿®æ”¹ä»£ç¢¼ï¼Œå¯ä»¥å…ˆæ¸›å°‘é€²åº¦æ¢è¼¸å‡ºï¼š

**ä¿®æ”¹ CKIPTokenizerOptimized**

```python
# In src/ir/text/ckip_tokenizer_optimized.py
def tokenize_batch(self, texts: List[str], ...):
    # æ¸›å°‘é€²åº¦æ¢è¼¸å‡ºé »ç‡
    batch_results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]

        # åªåœ¨æ¯ 1000 å€‹æ–‡æª”æ™‚é¡¯ç¤ºé€²åº¦
        disable_progress = (i % 1000 != 0)

        tokenized = self.ws(
            batch,
            batch_size=batch_size,
            show_progress=not disable_progress  # æ¸›å°‘è¼¸å‡º
        )
        batch_results.extend(tokenized)
```

---

## ğŸ“Š é æœŸæ•ˆèƒ½æ”¹å–„

### æ–¹æ¡ˆ A: æ‰¹æ¬¡è™•ç†

| æŒ‡æ¨™ | ç•¶å‰ | å„ªåŒ–å¾Œ | æ”¹å–„ |
|------|------|--------|------|
| **ç´¢å¼•å»ºç«‹æ™‚é–“** | 4.5 å°æ™‚ | **~45 åˆ†é˜** | **6x åŠ é€Ÿ** |
| **Tokenization å‘¼å«æ¬¡æ•¸** | 136,832 æ¬¡ | **500 æ¬¡** (50K/100) | **274x æ¸›å°‘** |
| **é€²åº¦æ¢æ—¥èªŒè¡Œæ•¸** | 273,000 è¡Œ | **~1,000 è¡Œ** | **273x æ¸›å°‘** |
| **I/O æ•ˆç‡** | ä½ (é€ä¸€è®€å–) | **é«˜** (æ‰¹æ¬¡ç·©å­˜) | é¡¯è‘—æå‡ |
| **è¨˜æ†¶é«”ä½¿ç”¨** | 1.3 GB | ~2.5 GB | å¯æ¥å— (+1.2 GB) |

### æ–¹æ¡ˆ B: æ¸›å°‘æ—¥èªŒ

| æŒ‡æ¨™ | ç•¶å‰ | å„ªåŒ–å¾Œ | æ”¹å–„ |
|------|------|--------|------|
| **ç´¢å¼•å»ºç«‹æ™‚é–“** | 4.5 å°æ™‚ | **~3.5 å°æ™‚** | **1.3x åŠ é€Ÿ** |
| **é€²åº¦æ¢æ—¥èªŒè¡Œæ•¸** | 273,000 è¡Œ | **~500 è¡Œ** | **546x æ¸›å°‘** |

---

## ğŸ”§ å¯¦ä½œæ­¥é©Ÿ (æ–¹æ¡ˆ A)

### Step 1: ä¿®æ”¹ IncrementalIndexBuilder

```bash
# å‚™ä»½åŸå§‹æ–‡ä»¶
cp src/ir/index/incremental_builder.py src/ir/index/incremental_builder.py.backup

# æ·»åŠ  add_documents_batch æ–¹æ³•
# (ç·¨è¼¯ src/ir/index/incremental_builder.py)
```

### Step 2: ä¿®æ”¹ UnifiedSearchEngine

```bash
# å‚™ä»½åŸå§‹æ–‡ä»¶
cp src/ir/search/unified_search.py src/ir/search/unified_search.py.backup

# ä¿®æ”¹ build_index_from_jsonl ä½¿ç”¨æ‰¹æ¬¡è™•ç†
# (ç·¨è¼¯ src/ir/search/unified_search.py)
```

### Step 3: æ¸¬è©¦æ‰¹æ¬¡è™•ç†

```bash
# å°è¦æ¨¡æ¸¬è©¦ (1000 ç¯‡)
python scripts/search_news.py \
    --build \
    --data-dir data/raw \
    --limit 1000 \
    --index-dir data/index_batch_test \
    --ckip-model bert-base

# æª¢æŸ¥æ•ˆèƒ½æ”¹å–„
# æ‡‰è©²åœ¨ 2-3 åˆ†é˜å…§å®Œæˆ (vs åŸæœ¬ ~10 åˆ†é˜)
```

### Step 4: å®Œæ•´ç´¢å¼•å»ºç«‹

```bash
# è¨­å®šç’°å¢ƒè®Šæ•¸
export OMP_NUM_THREADS=32
export MKL_NUM_THREADS=32
export NUMEXPR_NUM_THREADS=32

# å»ºç«‹å®Œæ•´ç´¢å¼•
time python scripts/search_news.py \
    --build \
    --data-dir data/raw \
    --limit 50000 \
    --index-dir data/index_50k_optimized \
    --ckip-model bert-base
```

---

## ğŸ“ æ³¨æ„äº‹é …

### æ‰¹æ¬¡å¤§å°é¸æ“‡

| æ‰¹æ¬¡å¤§å° | è¨˜æ†¶é«”ä½¿ç”¨ | é€Ÿåº¦ | é©ç”¨å ´æ™¯ |
|---------|-----------|------|---------|
| **50** | ~1.5 GB | ä¸­ç­‰ | è¨˜æ†¶é«”æœ‰é™ |
| **100** | ~2.0 GB | å¿« | **æ¨è–¦** (å¹³è¡¡) |
| **500** | ~4.0 GB | æœ€å¿« | è¨˜æ†¶é«”å……è¶³ |

### CKIP Batch Size

| å…§éƒ¨æ‰¹æ¬¡ | CKIP è™•ç†é€Ÿåº¦ | å»ºè­°é…ç½® |
|---------|--------------|---------|
| **256** | æ¨™æº– | 16 threads |
| **512** | å¿« | **32 threads** (æ¨è–¦) |
| **1024** | æœ€å¿« | 32 threads + é«˜è¨˜æ†¶é«” |

### ç›£æ§æŒ‡æ¨™

```bash
# ç›£æ§ CPU ä½¿ç”¨ (æ‡‰è©²ç¶­æŒ 2500-3000%)
htop -p $(pgrep -f 'search_news.py')

# ç›£æ§è¨˜æ†¶é«” (æ‡‰è©² < 4GB)
ps -p $(pgrep -f 'search_news.py') -o %mem,rss

# æª¢æŸ¥æ—¥èªŒå¤§å° (æ‡‰è©²æ˜é¡¯æ¸›å°‘)
ls -lh /tmp/build_*_index.log
```

---

## ğŸ“ æŠ€è¡“åŸç†

### ç‚ºä»€éº¼æ‰¹æ¬¡è™•ç†æ›´å¿«ï¼Ÿ

#### 1. **æ¸›å°‘å‡½æ•¸å‘¼å«é–‹éŠ·**
```
å–®æ–‡æª”: 50,000 æ¬¡ Python â†’ C++ å‘¼å«
æ‰¹æ¬¡è™•ç†: 500 æ¬¡ Python â†’ C++ å‘¼å« (100 docs/batch)
é–‹éŠ·æ¸›å°‘: 100x
```

#### 2. **æ›´å¥½çš„ CPU ç·©å­˜åˆ©ç”¨**
```
å–®æ–‡æª”: ç·©å­˜å¤±æ•ˆç‡é«˜ (æ¯æ¬¡é‡æ–°è¼‰å…¥æ¨¡å‹)
æ‰¹æ¬¡è™•ç†: æ¨¡å‹ä¿æŒåœ¨ç·©å­˜ä¸­è™•ç†æ•´å€‹æ‰¹æ¬¡
ç·©å­˜å‘½ä¸­ç‡: ~40% â†’ ~85%
```

#### 3. **æ›´æœ‰æ•ˆçš„åŸ·è¡Œç·’ä¸¦è¡Œ**
```
å–®æ–‡æª”: 32 threads è™•ç† 1 å€‹æ–‡æª” (åŸ·è¡Œç·’é¤“æ­»)
æ‰¹æ¬¡è™•ç†: 32 threads ä¸¦è¡Œè™•ç† 100 å€‹æ–‡æª”
åŸ·è¡Œç·’åˆ©ç”¨ç‡: ~30% â†’ ~90%
```

#### 4. **æ¸›å°‘é€²åº¦æ¢ I/O**
```
å–®æ–‡æª”: 136,832 æ¬¡ I/O å¯«å…¥ (é€²åº¦æ¢)
æ‰¹æ¬¡è™•ç†: 500 æ¬¡ I/O å¯«å…¥
I/O æ¸›å°‘: 274x
```

---

## ğŸš€ æœªä¾†æ”¹é€²æ–¹å‘

### 1. å¤šé€²ç¨‹ä¸¦è¡Œ (é€²éš)

```python
from multiprocessing import Pool

def process_batch(batch):
    # æ¯å€‹é€²ç¨‹è™•ç†ä¸€å€‹æ‰¹æ¬¡
    return index_builder.add_documents_batch(batch)

with Pool(processes=4) as pool:
    # 4 é€²ç¨‹ä¸¦è¡Œè™•ç†
    results = pool.map(process_batch, doc_batches)
```

**é æœŸ**: å†å¿« 2-3x (ä½†éœ€è¦æ›´å¤šè¨˜æ†¶é«”)

### 2. ç•°æ­¥ I/O (é€²éš)

```python
import asyncio

async def async_read_and_process():
    # ç•°æ­¥è®€å–å’Œè™•ç†
    async for doc_batch in async_doc_reader(...):
        await process_batch_async(doc_batch)
```

**é æœŸ**: I/O å’Œè¨ˆç®—å®Œå…¨é‡ç–Šï¼Œå†å¿« 1.5x

### 3. GPU åŠ é€Ÿ (é¸é …)

```python
# ä½¿ç”¨ GPU ç‰ˆæœ¬çš„ CKIP
tokenizer = get_optimized_tokenizer(use_gpu=True, num_threads=32)
```

**é æœŸ**: CKIP åˆ†è©å†å¿« 3-5x (éœ€è¦ GPU)

---

## ğŸ“ˆ æ•ˆèƒ½åŸºæº–æ¸¬è©¦

### æ¸¬è©¦ç’°å¢ƒ
- **CPU**: AMD Ryzen 9 9950X (32 threads)
- **è¨˜æ†¶é«”**: 64 GB DDR5
- **å­˜å„²**: NVMe SSD (WSL2)
- **æ–‡æª”**: 50,000 ç¯‡æ–°è (å¹³å‡ 500 tokens/ç¯‡)

### æ¸¬è©¦çµæœ (é æœŸ)

| é…ç½® | æ™‚é–“ | ååé‡ | CPU | è¨˜æ†¶é«” |
|------|------|--------|-----|--------|
| **åŸç‰ˆ (å–®æ–‡æª”)** | 4.5 å°æ™‚ | 3.1 docs/sec | 2961% | 1.3 GB |
| **æ–¹æ¡ˆ B (æ¸›å°‘æ—¥èªŒ)** | 3.5 å°æ™‚ | 4.0 docs/sec | 2961% | 1.3 GB |
| **æ–¹æ¡ˆ A (æ‰¹æ¬¡=100)** | **45 åˆ†é˜** | **18.5 docs/sec** | 3000% | 2.5 GB |
| **æ–¹æ¡ˆ A (æ‰¹æ¬¡=500)** | **30 åˆ†é˜** | **27.8 docs/sec** | 3100% | 4.0 GB |

---

## âœ… æª¢æŸ¥æ¸…å–®

### å„ªåŒ–å‰
- [x] åˆ†æç•¶å‰æ•ˆèƒ½ç“¶é ¸
- [x] ç¢ºèª CKIP tokenizer æ”¯æ´æ‰¹æ¬¡è™•ç†
- [x] è¨˜éŒ„ç•¶å‰æ•ˆèƒ½åŸºæº–

### å„ªåŒ–ä¸­
- [ ] å‚™ä»½åŸå§‹ä»£ç¢¼
- [ ] å¯¦ä½œæ‰¹æ¬¡è™•ç†æ–¹æ³•
- [ ] å°è¦æ¨¡æ¸¬è©¦ (1000 docs)
- [ ] é©—è­‰æ­£ç¢ºæ€§

### å„ªåŒ–å¾Œ
- [ ] å®Œæ•´ç´¢å¼•æ¸¬è©¦ (50K docs)
- [ ] æ•ˆèƒ½æ•¸æ“šæ”¶é›†
- [ ] èˆ‡åŸç‰ˆæ¯”è¼ƒ
- [ ] æ›´æ–°æ–‡æª”

---

**æ–‡æª”æ—¥æœŸ**: 2025-11-20
**ç‹€æ…‹**: å„ªåŒ–æ–¹æ¡ˆå·²åˆ¶å®šï¼Œå¾…å¯¦ä½œ
**é æœŸæ”¹å–„**: **6x åŠ é€Ÿ** (4.5 å°æ™‚ â†’ 45 åˆ†é˜)
**å„ªå…ˆç´š**: é«˜ (ä¸‹ä¸€å€‹ç´¢å¼•å»ºç«‹æ™‚æ¡ç”¨)
