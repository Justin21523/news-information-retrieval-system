"""
Comprehensive IR System Test Suite
ç¶œåˆè³‡è¨Šæª¢ç´¢ç³»çµ±æ¸¬è©¦å¥—ä»¶

è‡ªå‹•åŒ–æ¸¬è©¦æ‰€æœ‰æ¨¡çµ„åŠŸèƒ½ï¼Œä¸¦ç”Ÿæˆè©³ç´°æ¸¬è©¦å ±å‘Šã€‚
ä¸ä½¿ç”¨ pytestï¼Œä»¥æ¸…æ™°çš„æ ¼å¼å‘ˆç¾æ¸¬è©¦çµæœã€‚

Author: Information Retrieval System
Date: 2025-11-13
"""

import sys
import os
import time
import json
from datetime import datetime
from typing import Dict, List, Any, Tuple

# Add project root to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# Import all IR modules
from src.ir.text.chinese_tokenizer import ChineseTokenizer
from src.ir.index.inverted_index import InvertedIndex
from src.ir.index.positional_index import PositionalIndex
from src.ir.index.field_indexer import FieldIndexer
from src.ir.retrieval.boolean import BooleanQueryEngine
from src.ir.retrieval.vsm import VectorSpaceModel
from src.ir.retrieval.bm25 import BM25Ranker
from src.ir.retrieval.language_model_retrieval import LanguageModelRetrieval
from src.ir.retrieval.bim import BinaryIndependenceModel
from src.ir.retrieval.wildcard import WildcardExpander
from src.ir.retrieval.fuzzy import FuzzyMatcher
from src.ir.langmodel.ngram import NGramModel
from src.ir.langmodel.collocation import CollocationExtractor
from src.ir.ranking.rocchio import RocchioExpander
from src.ir.ranking.hybrid import HybridRanker
from src.ir.cluster.doc_cluster import DocumentClusterer
from src.ir.summarize.static import StaticSummarizer
from src.ir.summarize.dynamic import KWICGenerator
from src.ir.index.compression import VByteEncoder, GammaEncoder, DeltaEncoder


class TestResults:
    """æ¸¬è©¦çµæœè¨˜éŒ„å™¨"""

    def __init__(self):
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
        self.results = []
        self.start_time = time.time()

    def record(self, module: str, test_name: str, passed: bool,
               message: str = "", execution_time: float = 0):
        """è¨˜éŒ„å–®å€‹æ¸¬è©¦çµæœ"""
        self.total_tests += 1
        if passed:
            self.passed_tests += 1
            status = "âœ… PASS"
        else:
            self.failed_tests += 1
            status = "âŒ FAIL"

        self.results.append({
            'module': module,
            'test_name': test_name,
            'status': status,
            'passed': passed,
            'message': message,
            'execution_time': execution_time
        })

    def print_summary(self):
        """æ‰“å°æ¸¬è©¦ç¸½çµ"""
        total_time = time.time() - self.start_time

        print("\n" + "=" * 80)
        print("æ¸¬è©¦ç¸½çµ (Test Summary)")
        print("=" * 80)
        print(f"ç¸½æ¸¬è©¦æ•¸: {self.total_tests}")
        print(f"é€šé: {self.passed_tests} âœ…")
        print(f"å¤±æ•—: {self.failed_tests} âŒ")
        print(f"æˆåŠŸç‡: {(self.passed_tests/self.total_tests*100):.1f}%")
        print(f"ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f}s")
        print("=" * 80)

    def print_detailed_results(self):
        """æ‰“å°è©³ç´°æ¸¬è©¦çµæœ"""
        print("\n" + "=" * 80)
        print("è©³ç´°æ¸¬è©¦çµæœ (Detailed Test Results)")
        print("=" * 80)

        current_module = None
        for result in self.results:
            if result['module'] != current_module:
                current_module = result['module']
                print(f"\nã€{current_module}ã€‘")
                print("-" * 80)

            print(f"  {result['status']} {result['test_name']}")
            if result['message']:
                print(f"       â†’ {result['message']}")
            if result['execution_time'] > 0:
                print(f"       â± {result['execution_time']:.3f}s")

    def save_report(self, filepath: str = "test_report.json"):
        """ä¿å­˜æ¸¬è©¦å ±å‘Šç‚º JSON"""
        report = {
            'summary': {
                'total_tests': self.total_tests,
                'passed_tests': self.passed_tests,
                'failed_tests': self.failed_tests,
                'success_rate': self.passed_tests / self.total_tests if self.total_tests > 0 else 0,
                'total_time': time.time() - self.start_time,
                'timestamp': datetime.now().isoformat()
            },
            'results': self.results
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“„ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜: {filepath}")


class ComprehensiveTest:
    """ç¶œåˆæ¸¬è©¦é¡"""

    def __init__(self):
        self.results = TestResults()
        self.test_data = self._load_test_data()
        self.tokenizer = None

    def _load_test_data(self) -> Dict[str, Any]:
        """è¼‰å…¥æ¸¬è©¦è³‡æ–™"""
        return {
            'documents': [
                "è³‡è¨Šæª¢ç´¢æ˜¯å¾å¤§é‡è³‡æ–™ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Šçš„éç¨‹",
                "æª¢ç´¢æ¨¡å‹åŒ…æ‹¬å¸ƒæ—æ¨¡å‹å’Œå‘é‡ç©ºé–“æ¨¡å‹",
                "BM25æ˜¯ä¸€ç¨®æ©Ÿç‡æª¢ç´¢å‡½æ•¸",
                "æœå°‹å¼•æ“ä½¿ç”¨å„ç¨®æ’åºæ¼”ç®—æ³•åŒ…æ‹¬BM25",
                "èªè¨€æ¨¡å‹ä¼°è¨ˆè©åºåˆ—çš„æ©Ÿç‡",
                "äººå·¥æ™ºæ…§å’Œæ©Ÿå™¨å­¸ç¿’åœ¨è³‡è¨Šæª¢ç´¢ä¸­çš„æ‡‰ç”¨",
                "æ·±åº¦å­¸ç¿’æ¨¡å‹å¦‚BERTå¯ç”¨æ–¼èªç¾©æª¢ç´¢",
                "å°ç£çš„ç§‘æŠ€ç”¢æ¥­ç™¼å±•è¿…é€Ÿ",
                "ç¶“æ¿Ÿæˆé•·èˆ‡æŠ€è¡“å‰µæ–°å¯†åˆ‡ç›¸é—œ",
                "è‡ªç„¶èªè¨€è™•ç†æ˜¯äººå·¥æ™ºæ…§çš„é‡è¦é ˜åŸŸ"
            ],
            'articles': [
                {
                    'id': i,
                    'title': f"æ¸¬è©¦æ–‡ç«  {i}",
                    'content': doc,
                    'category': 'ç§‘æŠ€' if i % 2 == 0 else 'è²¡ç¶“',
                    'published_date': f"2025-11-{10+i:02d}",
                    'tags': ['æ¸¬è©¦', 'IR'],
                    'author': 'æ¸¬è©¦ä½œè€…',
                    'url': f'http://test.com/{i}'
                }
                for i, doc in enumerate([
                    "è³‡è¨Šæª¢ç´¢æ˜¯å¾å¤§é‡è³‡æ–™ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Šçš„éç¨‹",
                    "æª¢ç´¢æ¨¡å‹åŒ…æ‹¬å¸ƒæ—æ¨¡å‹å’Œå‘é‡ç©ºé–“æ¨¡å‹",
                    "BM25æ˜¯ä¸€ç¨®æ©Ÿç‡æª¢ç´¢å‡½æ•¸",
                    "æœå°‹å¼•æ“ä½¿ç”¨å„ç¨®æ’åºæ¼”ç®—æ³•åŒ…æ‹¬BM25",
                    "èªè¨€æ¨¡å‹ä¼°è¨ˆè©åºåˆ—çš„æ©Ÿç‡",
                    "äººå·¥æ™ºæ…§å’Œæ©Ÿå™¨å­¸ç¿’åœ¨è³‡è¨Šæª¢ç´¢ä¸­çš„æ‡‰ç”¨",
                    "æ·±åº¦å­¸ç¿’æ¨¡å‹å¦‚BERTå¯ç”¨æ–¼èªç¾©æª¢ç´¢",
                    "å°ç£çš„ç§‘æŠ€ç”¢æ¥­ç™¼å±•è¿…é€Ÿ",
                    "ç¶“æ¿Ÿæˆé•·èˆ‡æŠ€è¡“å‰µæ–°å¯†åˆ‡ç›¸é—œ",
                    "è‡ªç„¶èªè¨€è™•ç†æ˜¯äººå·¥æ™ºæ…§çš„é‡è¦é ˜åŸŸ"
                ])
            ],
            'queries': [
                "è³‡è¨Šæª¢ç´¢",
                "BM25æ’åº",
                "äººå·¥æ™ºæ…§",
                "å°ç£ç¶“æ¿Ÿ"
            ]
        }

    def run_all_tests(self):
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print("=" * 80)
        print("é–‹å§‹ç¶œåˆæ¸¬è©¦ (Starting Comprehensive Tests)")
        print("=" * 80)
        print(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ¸¬è©¦æ–‡æª”æ•¸: {len(self.test_data['documents'])}")
        print(f"æ¸¬è©¦æŸ¥è©¢æ•¸: {len(self.test_data['queries'])}")
        print("=" * 80)

        # 1. æ–‡æœ¬è™•ç†æ¸¬è©¦
        self.test_text_processing()

        # 2. ç´¢å¼•æ¸¬è©¦
        self.test_indexing()

        # 3. æª¢ç´¢æ¨¡å‹æ¸¬è©¦
        self.test_retrieval_models()

        # 4. èªè¨€æ¨¡å‹æ¸¬è©¦
        self.test_language_models()

        # 5. é€²éšåŠŸèƒ½æ¸¬è©¦
        self.test_advanced_features()

        # 6. å£“ç¸®æ¸¬è©¦
        self.test_compression()

        # æ‰“å°çµæœ
        self.results.print_detailed_results()
        self.results.print_summary()

        # ä¿å­˜å ±å‘Š
        self.results.save_report('test_report.json')

    def test_text_processing(self):
        """æ¸¬è©¦æ–‡æœ¬è™•ç†æ¨¡çµ„"""
        module = "æ–‡æœ¬è™•ç† (Text Processing)"

        # Test 1: ChineseTokenizer åˆå§‹åŒ–
        try:
            start = time.time()
            self.tokenizer = ChineseTokenizer(engine='ckip', use_pos=True, device=-1)
            exec_time = time.time() - start
            self.results.record(module, "CKIP Tokenizer åˆå§‹åŒ–", True,
                              f"æˆåŠŸè¼‰å…¥ CKIP æ¨¡å‹", exec_time)
        except Exception as e:
            self.results.record(module, "CKIP Tokenizer åˆå§‹åŒ–", False, f"éŒ¯èª¤: {e}")
            return

        # Test 2: åˆ†è©æ¸¬è©¦
        try:
            test_text = "è³‡è¨Šæª¢ç´¢ç³»çµ±"
            tokens = self.tokenizer.tokenize(test_text)
            passed = len(tokens) > 0
            self.results.record(module, "ä¸­æ–‡åˆ†è©", passed,
                              f"è¼¸å…¥: '{test_text}' â†’ åˆ†è©: {tokens}")
        except Exception as e:
            self.results.record(module, "ä¸­æ–‡åˆ†è©", False, f"éŒ¯èª¤: {e}")

        # Test 3: è©æ€§æ¨™æ³¨æ¸¬è©¦
        try:
            test_text = "äººå·¥æ™ºæ…§ç™¼å±•"
            tokens_pos = self.tokenizer.tokenize_with_pos(test_text)
            passed = len(tokens_pos) > 0 and all(len(tp) == 2 for tp in tokens_pos)
            self.results.record(module, "è©æ€§æ¨™æ³¨ (POS)", passed,
                              f"çµæœ: {tokens_pos}")
        except Exception as e:
            self.results.record(module, "è©æ€§æ¨™æ³¨ (POS)", False, f"éŒ¯èª¤: {e}")

        # Test 4: å‘½åå¯¦é«”è­˜åˆ¥æ¸¬è©¦
        try:
            test_text = "å°ç£çš„ç§‘æŠ€ç”¢æ¥­ç™¼å±•"
            entities = self.tokenizer.extract_entities(test_text)
            self.results.record(module, "å‘½åå¯¦é«”è­˜åˆ¥ (NER)", True,
                              f"è­˜åˆ¥å¯¦é«”: {entities if entities else 'ç„¡'}")
        except Exception as e:
            self.results.record(module, "å‘½åå¯¦é«”è­˜åˆ¥ (NER)", False, f"éŒ¯èª¤: {e}")

    def test_indexing(self):
        """æ¸¬è©¦ç´¢å¼•æ¨¡çµ„"""
        module = "ç´¢å¼• (Indexing)"

        docs = self.test_data['documents']

        # Test 1: Inverted Index
        try:
            start = time.time()
            inv_idx = InvertedIndex(tokenizer=self.tokenizer.tokenize)
            inv_idx.build(docs)
            exec_time = time.time() - start

            vocab_size = len(inv_idx.vocabulary)
            passed = vocab_size > 0
            self.results.record(module, "å€’æ’ç´¢å¼• (Inverted Index)", passed,
                              f"è©å½™é‡: {vocab_size}, æ–‡æª”æ•¸: {len(docs)}", exec_time)
        except Exception as e:
            self.results.record(module, "å€’æ’ç´¢å¼• (Inverted Index)", False, f"éŒ¯èª¤: {e}")

        # Test 2: Positional Index
        try:
            start = time.time()
            pos_idx = PositionalIndex(tokenizer=self.tokenizer.tokenize)
            pos_idx.build(docs)
            exec_time = time.time() - start

            # æ¸¬è©¦ä½ç½®æŸ¥è©¢
            positions = pos_idx.get_positions("æª¢ç´¢", 0)
            passed = positions is not None
            self.results.record(module, "ä½ç½®ç´¢å¼• (Positional Index)", passed,
                              f"'æª¢ç´¢' åœ¨ doc_0 çš„ä½ç½®: {positions}", exec_time)
        except Exception as e:
            self.results.record(module, "ä½ç½®ç´¢å¼• (Positional Index)", False, f"éŒ¯èª¤: {e}")

        # Test 3: Field Indexer
        try:
            start = time.time()
            field_idx = FieldIndexer(tokenizer=self.tokenizer.tokenize)
            field_idx.build(self.test_data['articles'])
            exec_time = time.time() - start

            # æ¸¬è©¦æ¬„ä½æŸ¥è©¢
            results = field_idx.search_field('category', 'ç§‘æŠ€')
            passed = len(results) > 0
            self.results.record(module, "æ¬„ä½ç´¢å¼• (Field Index)", passed,
                              f"category='ç§‘æŠ€' æ‰¾åˆ° {len(results)} ç¯‡æ–‡æª”", exec_time)
        except Exception as e:
            self.results.record(module, "æ¬„ä½ç´¢å¼• (Field Index)", False, f"éŒ¯èª¤: {e}")

    def test_retrieval_models(self):
        """æ¸¬è©¦æª¢ç´¢æ¨¡å‹"""
        module = "æª¢ç´¢æ¨¡å‹ (Retrieval Models)"

        docs = self.test_data['documents']
        query = "è³‡è¨Šæª¢ç´¢"

        # Test 1: VSM (Vector Space Model)
        try:
            start = time.time()
            vsm = VectorSpaceModel()
            vsm.build_index(docs)
            result = vsm.search(query, topk=3)
            exec_time = time.time() - start

            passed = result.num_results > 0
            top_score = result.scores[0] if result.scores else 0
            self.results.record(module, "VSM (TF-IDF)", passed,
                              f"æŸ¥è©¢: '{query}' â†’ æ‰¾åˆ° {result.num_results} ç¯‡, Top score: {top_score:.4f}",
                              exec_time)
        except Exception as e:
            self.results.record(module, "VSM (TF-IDF)", False, f"éŒ¯èª¤: {e}")

        # Test 2: BM25
        try:
            start = time.time()
            bm25 = BM25Ranker(tokenizer=self.tokenizer.tokenize, k1=1.5, b=0.75)
            bm25.build_index(docs)
            result = bm25.search(query, topk=3)
            exec_time = time.time() - start

            passed = result.num_results > 0
            top_score = result.scores[0] if result.scores else 0
            self.results.record(module, "BM25 Ranking", passed,
                              f"æŸ¥è©¢: '{query}' â†’ æ‰¾åˆ° {result.num_results} ç¯‡, Top score: {top_score:.4f}",
                              exec_time)
        except Exception as e:
            self.results.record(module, "BM25 Ranking", False, f"éŒ¯èª¤: {e}")

        # Test 3: Language Model Retrieval
        try:
            start = time.time()
            lm = LanguageModelRetrieval(
                tokenizer=self.tokenizer.tokenize,
                smoothing='dirichlet',
                mu_param=2000
            )
            lm.build_index(docs)
            result = lm.search(query, topk=3)
            exec_time = time.time() - start

            passed = result.num_results > 0
            top_score = result.scores[0] if result.scores else 0
            self.results.record(module, "Language Model Retrieval", passed,
                              f"æŸ¥è©¢: '{query}' â†’ æ‰¾åˆ° {result.num_results} ç¯‡, Top score: {top_score:.4f}",
                              exec_time)
        except Exception as e:
            self.results.record(module, "Language Model Retrieval", False, f"éŒ¯èª¤: {e}")

        # Test 4: BIM (Binary Independence Model)
        try:
            start = time.time()
            bim = BinaryIndependenceModel(tokenizer=self.tokenizer.tokenize, use_idf=True)
            bim.build_index(docs)
            result = bim.search(query, topk=3)
            exec_time = time.time() - start

            passed = result.num_results > 0
            top_score = result.scores[0] if result.scores else 0
            self.results.record(module, "BIM (Binary Independence Model)", passed,
                              f"æŸ¥è©¢: '{query}' â†’ æ‰¾åˆ° {result.num_results} ç¯‡, RSV: {top_score:.4f}",
                              exec_time)
        except Exception as e:
            self.results.record(module, "BIM (Binary Independence Model)", False, f"éŒ¯èª¤: {e}")

        # Test 5: Boolean Query
        try:
            start = time.time()
            inv_idx = InvertedIndex(tokenizer=self.tokenizer.tokenize)
            inv_idx.build(docs)
            pos_idx = PositionalIndex(tokenizer=self.tokenizer.tokenize)
            pos_idx.build(docs)

            boolean_engine = BooleanQueryEngine(
                inverted_index=inv_idx,
                positional_index=pos_idx
            )

            # æ¸¬è©¦ AND æŸ¥è©¢
            result = boolean_engine.search("è³‡è¨Š AND æª¢ç´¢")
            exec_time = time.time() - start

            passed = len(result) >= 0
            self.results.record(module, "Boolean Query (AND)", passed,
                              f"'è³‡è¨Š AND æª¢ç´¢' â†’ æ‰¾åˆ° {len(result)} ç¯‡", exec_time)
        except Exception as e:
            self.results.record(module, "Boolean Query (AND)", False, f"éŒ¯èª¤: {e}")

        # Test 6: NEAR Query
        try:
            result = boolean_engine.search("è³‡è¨Š NEAR/5 æª¢ç´¢")
            passed = len(result) >= 0
            self.results.record(module, "NEAR/n Query", passed,
                              f"'è³‡è¨Š NEAR/5 æª¢ç´¢' â†’ æ‰¾åˆ° {len(result)} ç¯‡")
        except Exception as e:
            self.results.record(module, "NEAR/n Query", False, f"éŒ¯èª¤: {e}")

        # Test 7: Hybrid Ranking
        try:
            start = time.time()
            # æº–å‚™å¤šå€‹ rankers
            vsm_ranker = VectorSpaceModel()
            vsm_ranker.build_index(docs)

            bm25_ranker = BM25Ranker(tokenizer=self.tokenizer.tokenize)
            bm25_ranker.build_index(docs)

            lm_ranker = LanguageModelRetrieval(tokenizer=self.tokenizer.tokenize)
            lm_ranker.build_index(docs)

            hybrid = HybridRanker(
                rankers={'vsm': vsm_ranker, 'bm25': bm25_ranker, 'lm': lm_ranker},
                fusion_method='rrf',
                normalization='minmax'
            )

            result = hybrid.search(query, topk=3)
            exec_time = time.time() - start

            passed = result.num_results > 0
            top_score = result.scores[0] if result.scores else 0
            self.results.record(module, "Hybrid Ranking (RRF)", passed,
                              f"æŸ¥è©¢: '{query}' â†’ èåˆ 3 å€‹æ¨¡å‹, Top score: {top_score:.4f}",
                              exec_time)
        except Exception as e:
            self.results.record(module, "Hybrid Ranking (RRF)", False, f"éŒ¯èª¤: {e}")

    def test_language_models(self):
        """æ¸¬è©¦èªè¨€æ¨¡å‹"""
        module = "èªè¨€æ¨¡å‹ (Language Models)"

        docs = self.test_data['documents']

        # Test 1: N-gram Model
        try:
            start = time.time()
            ngram = NGramModel(n=2, smoothing='jm', lambda_param=0.7,
                             tokenizer=self.tokenizer.tokenize)
            ngram.train(docs)
            exec_time = time.time() - start

            stats = ngram.get_stats()
            self.results.record(module, "N-gram Model (Bigram)", True,
                              f"è©å½™: {stats['vocabulary_size']}, è¨“ç·´æ–‡æª”: {len(docs)}", exec_time)
        except Exception as e:
            self.results.record(module, "N-gram Model (Bigram)", False, f"éŒ¯èª¤: {e}")

        # Test 2: N-gram Perplexity
        try:
            test_text = "è³‡è¨Šæª¢ç´¢ç³»çµ±"
            perplexity = ngram.perplexity(test_text)
            passed = perplexity > 0 and perplexity < float('inf')
            self.results.record(module, "N-gram Perplexity", passed,
                              f"æ–‡æœ¬: '{test_text}' â†’ Perplexity: {perplexity:.2f}")
        except Exception as e:
            self.results.record(module, "N-gram Perplexity", False, f"éŒ¯èª¤: {e}")

        # Test 3: Collocation Extraction
        try:
            start = time.time()
            colloc = CollocationExtractor(
                tokenizer=self.tokenizer.tokenize,
                min_freq=1,
                window_size=2
            )
            colloc.train(docs)

            # æå– top collocations
            collocations = colloc.extract_collocations(measure='pmi', topk=5)
            exec_time = time.time() - start

            passed = len(collocations) > 0
            top_bigrams = [' '.join(c.bigram) for c in collocations[:3]]
            self.results.record(module, "Collocation Extraction (PMI)", passed,
                              f"æå– {len(collocations)} å€‹è©å½™çµ„åˆ, Top 3: {top_bigrams}",
                              exec_time)
        except Exception as e:
            self.results.record(module, "Collocation Extraction (PMI)", False, f"éŒ¯èª¤: {e}")

        # Test 4: Multiple Collocation Measures
        try:
            measures = ['llr', 'chi_square', 't_score', 'dice']
            for measure in measures:
                collocations = colloc.extract_collocations(measure=measure, topk=3)
                passed = len(collocations) > 0
                self.results.record(module, f"Collocation ({measure.upper()})", passed,
                                  f"æå– {len(collocations)} å€‹çµ„åˆ")
        except Exception as e:
            self.results.record(module, "Collocation (Multiple Measures)", False, f"éŒ¯èª¤: {e}")

    def test_advanced_features(self):
        """æ¸¬è©¦é€²éšåŠŸèƒ½"""
        module = "é€²éšåŠŸèƒ½ (Advanced Features)"

        docs = self.test_data['documents']

        # Test 1: Wildcard Query
        try:
            vocab = set()
            for doc in docs:
                tokens = self.tokenizer.tokenize(doc)
                vocab.update(tokens)

            wildcard = WildcardExpander(vocabulary=vocab)
            matches = wildcard.expand("æª¢*")
            passed = len(matches) > 0
            self.results.record(module, "Wildcard Query (æª¢*)", passed,
                              f"åŒ¹é…è©å½™: {matches[:5]}")
        except Exception as e:
            self.results.record(module, "Wildcard Query (æª¢*)", False, f"éŒ¯èª¤: {e}")

        # Test 2: Fuzzy Query
        try:
            fuzzy = FuzzyMatcher(max_distance=2, max_expansions=50)
            matches = fuzzy.fuzzy_match("æª¢ç´¢", vocab, max_distance=1)
            passed = len(matches) > 0
            match_terms = [term for term, dist in matches[:3]]
            self.results.record(module, "Fuzzy Query (æª¢ç´¢~1)", passed,
                              f"æ‰¾åˆ° {len(matches)} å€‹ç›¸ä¼¼è©: {match_terms}")
        except Exception as e:
            self.results.record(module, "Fuzzy Query (æª¢ç´¢~1)", False, f"éŒ¯èª¤: {e}")

        # Test 3: Query Expansion (Rocchio)
        try:
            vsm = VectorSpaceModel()
            vsm.build_index(docs)

            # ç²å–æŸ¥è©¢å‘é‡
            query_tokens = self.tokenizer.tokenize("è³‡è¨Šæª¢ç´¢")
            query_vector = {term: 1.0 for term in query_tokens}

            # ç²å–ç›¸é—œæ–‡æª”å‘é‡
            result = vsm.search("è³‡è¨Šæª¢ç´¢", topk=3)
            relevant_vectors = [vsm.get_document_vector(doc_id) for doc_id in result.doc_ids]

            # Rocchio æ“´å±•
            rocchio = RocchioExpander()
            expanded = rocchio.expand_query(query_vector, relevant_vectors)

            passed = len(expanded.expanded_vector) > len(query_vector)
            expansion_terms = list(set(expanded.expanded_vector.keys()) - set(query_vector.keys()))[:3]
            self.results.record(module, "Query Expansion (Rocchio)", passed,
                              f"åŸå§‹æŸ¥è©¢è©æ•¸: {len(query_vector)}, æ“´å±•å¾Œ: {len(expanded.expanded_vector)}, "
                              f"æ–°å¢è©: {expansion_terms}")
        except Exception as e:
            self.results.record(module, "Query Expansion (Rocchio)", False, f"éŒ¯èª¤: {e}")

        # Test 4: Document Clustering
        try:
            start = time.time()
            clusterer = DocumentClusterer()
            clusters = clusterer.cluster(docs, n_clusters=3, method='hierarchical')
            exec_time = time.time() - start

            passed = len(clusters) > 0
            cluster_sizes = [len(c.doc_ids) for c in clusters]
            self.results.record(module, "Document Clustering (Hierarchical)", passed,
                              f"åˆ†æˆ {len(clusters)} ç¾¤, ç¾¤å¤§å°: {cluster_sizes}", exec_time)
        except Exception as e:
            self.results.record(module, "Document Clustering (Hierarchical)", False, f"éŒ¯èª¤: {e}")

        # Test 5: Summarization - Lead-K
        try:
            summarizer = StaticSummarizer()
            test_doc = docs[0]
            summary = summarizer.lead_k_summarization(test_doc, k=2)

            passed = len(summary.sentences) > 0
            self.results.record(module, "Summarization (Lead-K)", passed,
                              f"å¾æ–‡æª”æå– {len(summary.sentences)} å¥æ‘˜è¦")
        except Exception as e:
            self.results.record(module, "Summarization (Lead-K)", False, f"éŒ¯èª¤: {e}")

        # Test 6: KWIC Generator
        try:
            kwic = KWICGenerator()
            test_doc = "è³‡è¨Šæª¢ç´¢æ˜¯å¾å¤§é‡è³‡æ–™ä¸­æ‰¾åˆ°ç›¸é—œè³‡è¨Šçš„éç¨‹ï¼Œè³‡è¨Šæª¢ç´¢ç³»çµ±æ˜¯é‡è¦çš„å·¥å…·"
            result = kwic.generate(test_doc, "è³‡è¨Š")

            passed = len(result.contexts) > 0
            self.results.record(module, "KWIC (KeyWord In Context)", passed,
                              f"é—œéµè© 'è³‡è¨Š' å‡ºç¾ {result.occurrences} æ¬¡")
        except Exception as e:
            self.results.record(module, "KWIC (KeyWord In Context)", False, f"éŒ¯èª¤: {e}")

    def test_compression(self):
        """æ¸¬è©¦ç´¢å¼•å£“ç¸®"""
        module = "ç´¢å¼•å£“ç¸® (Index Compression)"

        test_doc_ids = [3, 7, 10, 15, 22, 30, 35, 50, 100, 200]

        # Test 1: VByte Encoding
        try:
            vbyte = VByteEncoder()
            encoded = vbyte.encode_gaps(test_doc_ids)
            decoded = vbyte.decode_gaps(encoded)

            passed = decoded == test_doc_ids
            original_size = len(test_doc_ids) * 4
            compressed_size = len(encoded)
            ratio = compressed_size / original_size

            self.results.record(module, "VByte Encoding", passed,
                              f"åŸå§‹: {original_size}B â†’ å£“ç¸®: {compressed_size}B "
                              f"(å£“ç¸®ç‡: {ratio:.2%})")
        except Exception as e:
            self.results.record(module, "VByte Encoding", False, f"éŒ¯èª¤: {e}")

        # Test 2: Gamma Encoding
        try:
            gamma = GammaEncoder()
            encoded = gamma.encode_gaps(test_doc_ids)
            decoded = gamma.decode_gaps(encoded)

            passed = decoded == test_doc_ids
            gamma_bytes = (len(encoded) + 7) // 8
            ratio = gamma_bytes / original_size

            self.results.record(module, "Gamma Encoding", passed,
                              f"åŸå§‹: {original_size}B â†’ å£“ç¸®: {gamma_bytes}B "
                              f"(å£“ç¸®ç‡: {ratio:.2%})")
        except Exception as e:
            self.results.record(module, "Gamma Encoding", False, f"éŒ¯èª¤: {e}")

        # Test 3: Delta Encoding
        try:
            delta = DeltaEncoder()
            encoded = delta.encode_gaps(test_doc_ids)
            decoded = delta.decode_gaps(encoded)

            passed = decoded == test_doc_ids
            delta_bytes = (len(encoded) + 7) // 8
            ratio = delta_bytes / original_size

            self.results.record(module, "Delta Encoding", passed,
                              f"åŸå§‹: {original_size}B â†’ å£“ç¸®: {delta_bytes}B "
                              f"(å£“ç¸®ç‡: {ratio:.2%})")
        except Exception as e:
            self.results.record(module, "Delta Encoding", False, f"éŒ¯èª¤: {e}")


def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("\n" + "ğŸš€" * 40)
    print("è³‡è¨Šæª¢ç´¢ç³»çµ± - ç¶œåˆæ¸¬è©¦")
    print("Information Retrieval System - Comprehensive Test Suite")
    print("ğŸš€" * 40 + "\n")

    # åŸ·è¡Œæ¸¬è©¦
    tester = ComprehensiveTest()
    tester.run_all_tests()

    print("\n" + "âœ¨" * 40)
    print("æ¸¬è©¦å®Œæˆ! (Tests Completed!)")
    print("âœ¨" * 40 + "\n")


if __name__ == '__main__':
    main()
